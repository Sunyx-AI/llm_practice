_dataset = dict(
    abbr='Xsum',
    dataset_name='Xsum',
    eval_cfg=dict(
        evaluator=dict(
            type='opencompass.openicl.icl_evaluator.RougeEvaluator'),
        pred_postprocessor=dict(type='Xsum'),
        pred_role='BOT'),
    infer_cfg=dict(
        inferencer=dict(
            type='opencompass.openicl.icl_inferencer.GenInferencer'),
        prompt_template=dict(
            template=dict(round=[
                dict(
                    prompt=
                    'Document：{dialogue}\nBased on the previous text, provide a brief single summary:',
                    role='HUMAN'),
            ]),
            type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
        retriever=dict(
            type='opencompass.openicl.icl_retriever.ZeroRetriever')),
    path='opencompass/xsum',
    reader_cfg=dict(input_columns=[
        'dialogue',
    ], output_column='summary'),
    type='opencompass.datasets.XsumDataset')
datasets = [
    dict(
        abbr='ARC-c',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            ),
            pred_role='BOT'),
        infer_cfg=dict(
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'Question: {question}\nA. {textA}\nB. {textB}\nC. {textC}\nD. {textD}\nAnswer:',
                        role='HUMAN'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        name='ARC-Challenge',
        path='opencompass/ai2_arc-dev',
        reader_cfg=dict(
            input_columns=[
                'question',
                'textA',
                'textB',
                'textC',
                'textD',
            ],
            output_column='answerKey',
            test_range=10),
        type='opencompass.datasets.ARCDataset'),
    dict(
        abbr='ceval-computer_network',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于计算机网络考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='computer_network',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-operating_system',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于操作系统考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='operating_system',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-computer_architecture',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于计算机组成考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='computer_architecture',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-college_programming',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于大学编程考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_programming',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-college_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于大学物理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_physics',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-college_chemistry',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于大学化学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_chemistry',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-advanced_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于高等数学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='advanced_mathematics',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-probability_and_statistics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于概率统计考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='probability_and_statistics',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-discrete_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于离散数学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='discrete_mathematics',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-electrical_engineer',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于注册电气工程师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='electrical_engineer',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-metrology_engineer',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于注册计量师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='metrology_engineer',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-high_school_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于高中数学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_mathematics',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-high_school_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于高中物理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_physics',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-high_school_chemistry',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于高中化学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_chemistry',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-high_school_biology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于高中生物考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_biology',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-middle_school_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于初中数学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_mathematics',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-middle_school_biology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于初中生物考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_biology',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-middle_school_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于初中物理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_physics',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-middle_school_chemistry',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于初中化学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_chemistry',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-veterinary_medicine',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于兽医学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='veterinary_medicine',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-college_economics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于大学经济学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_economics',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-business_administration',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于工商管理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='business_administration',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-marxism',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于马克思主义基本原理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='marxism',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-mao_zedong_thought',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于毛泽东思想和中国特色社会主义理论体系概论考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='mao_zedong_thought',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-education_science',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于教育学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='education_science',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-teacher_qualification',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于教师资格考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='teacher_qualification',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-high_school_politics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于高中政治考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_politics',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-high_school_geography',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于高中地理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_geography',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-middle_school_politics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于初中政治考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_politics',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-middle_school_geography',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于初中地理考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_geography',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-modern_chinese_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于近代史纲要考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='modern_chinese_history',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-ideological_and_moral_cultivation',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于思想道德修养与法律基础考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='ideological_and_moral_cultivation',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-logic',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于逻辑学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='logic',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-law',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于法学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='law',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-chinese_language_and_literature',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于中国语言文学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='chinese_language_and_literature',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-art_studies',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于艺术学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='art_studies',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-professional_tour_guide',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于导游资格考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='professional_tour_guide',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-legal_professional',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于法律职业资格考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='legal_professional',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-high_school_chinese',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于高中语文考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_chinese',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-high_school_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于高中历史考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_history',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-middle_school_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于初中历史考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='middle_school_history',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-civil_servant',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于公务员考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='civil_servant',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-sports_science',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于体育学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='sports_science',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-plant_protection',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于植物保护考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='plant_protection',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-basic_medicine',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于基础医学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='basic_medicine',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-clinical_medicine',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于临床医学考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='clinical_medicine',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-urban_and_rural_planner',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于注册城乡规划师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='urban_and_rural_planner',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-accountant',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于注册会计师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='accountant',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-fire_engineer',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于注册消防工程师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='fire_engineer',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-environmental_impact_assessment_engineer',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于环境影响评价工程师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='environmental_impact_assessment_engineer',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-tax_accountant',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于税务师考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='tax_accountant',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='ceval-physician',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccEvaluator'),
            pred_postprocessor=dict(
                type=
                'opencompass.utils.text_postprocessors.first_capital_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            '以下是中国关于医师资格考试的单项选择题，请选出其中的正确答案。\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\n答案: ',
                            role='HUMAN'),
                        dict(prompt='{answer}', role='BOT'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='physician',
        path='opencompass/ceval-exam',
        reader_cfg=dict(
            input_columns=[
                'question',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='answer',
            test_range=10,
            test_split='val',
            train_split='dev'),
        type='opencompass.datasets.CEvalDataset'),
    dict(
        abbr='gsm8k',
        eval_cfg=dict(
            dataset_postprocessor=dict(
                type='opencompass.datasets.gsm8k_dataset_postprocess'),
            evaluator=dict(type='opencompass.datasets.Gsm8kEvaluator'),
            pred_postprocessor=dict(
                type='opencompass.datasets.gsm8k_postprocess')),
        infer_cfg=dict(
            inferencer=dict(
                max_out_len=512,
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        "Question: Angelo and Melanie want to plan how many hours over the next week they should study together for their test next week. They have 2 chapters of their textbook to study and 4 worksheets to memorize. They figure out that they should dedicate 3 hours to each chapter of their textbook and 1.5 hours for each worksheet. If they plan to study no more than 4 hours each day, how many days should they plan to study total over the next week if they take a 10-minute break every hour, include 3 10-minute snack breaks each day, and 30 minutes for lunch each day?\nLet's think step by step\nAnswer:",
                        role='HUMAN'),
                    dict(
                        prompt=
                        'Angelo and Melanie think they should dedicate 3 hours to each of the 2 chapters, 3 hours x 2 chapters = 6 hours total.\nFor the worksheets they plan to dedicate 1.5 hours for each worksheet, 1.5 hours x 4 worksheets = 6 hours total.\nAngelo and Melanie need to start with planning 12 hours to study, at 4 hours a day, 12 / 4 = 3 days.\nHowever, they need to include time for breaks and lunch. Every hour they want to include a 10-minute break, so 12 total hours x 10 minutes = 120 extra minutes for breaks.\nThey also want to include 3 10-minute snack breaks, 3 x 10 minutes = 30 minutes.\nAnd they want to include 30 minutes for lunch each day, so 120 minutes for breaks + 30 minutes for snack breaks + 30 minutes for lunch = 180 minutes, or 180 / 60 minutes per hour = 3 extra hours.\nSo Angelo and Melanie want to plan 12 hours to study + 3 hours of breaks = 15 hours total.\nThey want to study no more than 4 hours each day, 15 hours / 4 hours each day = 3.75\nThey will need to plan to study 4 days to allow for all the time they need.\nThe answer is 4\n',
                        role='BOT'),
                    dict(
                        prompt=
                        "Question: Mark's basketball team scores 25 2 pointers, 8 3 pointers and 10 free throws.  Their opponents score double the 2 pointers but half the 3 pointers and free throws.  What's the total number of points scored by both teams added together?\nLet's think step by step\nAnswer:",
                        role='HUMAN'),
                    dict(
                        prompt=
                        "Mark's team scores 25 2 pointers, meaning they scored 25*2= 50 points in 2 pointers.\nHis team also scores 6 3 pointers, meaning they scored 8*3= 24 points in 3 pointers\nThey scored 10 free throws, and free throws count as one point so they scored 10*1=10 points in free throws.\nAll together his team scored 50+24+10= 84 points\nMark's opponents scored double his team's number of 2 pointers, meaning they scored 50*2=100 points in 2 pointers.\nHis opponents scored half his team's number of 3 pointers, meaning they scored 24/2= 12 points in 3 pointers.\nThey also scored half Mark's team's points in free throws, meaning they scored 10/2=5 points in free throws.\nAll together Mark's opponents scored 100+12+5=117 points\nThe total score for the game is both team's scores added together, so it is 84+117=201 points\nThe answer is 201\n",
                        role='BOT'),
                    dict(
                        prompt=
                        "Question: Bella has two times as many marbles as frisbees. She also has 20 more frisbees than deck cards. If she buys 2/5 times more of each item, what would be the total number of the items she will have if she currently has 60 marbles?\nLet's think step by step\nAnswer:",
                        role='HUMAN'),
                    dict(
                        prompt=
                        "When Bella buys 2/5 times more marbles, she'll have increased the number of marbles by 2/5*60 = 24\nThe total number of marbles she'll have is 60+24 = 84\nIf Bella currently has 60 marbles, and she has two times as many marbles as frisbees, she has 60/2 = 30 frisbees.\nIf Bella buys 2/5 times more frisbees, she'll have 2/5*30 = 12 more frisbees.\nThe total number of frisbees she'll have will increase to 30+12 = 42\nBella also has 20 more frisbees than deck cards, meaning she has 30-20 = 10 deck cards\nIf she buys 2/5 times more deck cards, she'll have 2/5*10 = 4 more deck cards.\nThe total number of deck cards she'll have is 10+4 = 14\nTogether, Bella will have a total of 14+42+84 = 140 items\nThe answer is 140\n",
                        role='BOT'),
                    dict(
                        prompt=
                        "Question: A group of 4 fruit baskets contains 9 apples, 15 oranges, and 14 bananas in the first three baskets and 2 less of each fruit in the fourth basket. How many fruits are there?\nLet's think step by step\nAnswer:",
                        role='HUMAN'),
                    dict(
                        prompt=
                        'For the first three baskets, the number of apples and oranges in one basket is 9+15=24\nIn total, together with bananas, the number of fruits in one basket is 24+14=38 for the first three baskets.\nSince there are three baskets each having 38 fruits, there are 3*38=114 fruits in the first three baskets.\nThe number of apples in the fourth basket is 9-2=7\nThere are also 15-2=13 oranges in the fourth basket\nThe combined number of oranges and apples in the fourth basket is 13+7=20\nThe fourth basket also contains 14-2=12 bananas.\nIn total, the fourth basket has 20+12=32 fruits.\nThe four baskets together have 32+114=146 fruits.\nThe answer is 146\n',
                        role='BOT'),
                    dict(
                        prompt=
                        "Question: {question}\nLet's think step by step\nAnswer:",
                        role='HUMAN'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                type='opencompass.openicl.icl_retriever.ZeroRetriever')),
        path='opencompass/gsm8k',
        reader_cfg=dict(
            input_columns=[
                'question',
            ],
            output_column='answer',
            test_range=10),
        type='opencompass.datasets.GSM8KDataset'),
    dict(
        abbr='lukaemon_mmlu_college_biology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about college biology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about college biology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_biology',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_college_chemistry',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about college chemistry. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about college chemistry. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_chemistry',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_college_computer_science',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about college computer science. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about college computer science. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_computer_science',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_college_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about college mathematics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about college mathematics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_mathematics',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_college_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about college physics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about college physics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_physics',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_electrical_engineering',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about electrical engineering. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about electrical engineering. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='electrical_engineering',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_astronomy',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about astronomy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about astronomy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='astronomy',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_anatomy',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about anatomy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about anatomy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='anatomy',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_abstract_algebra',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about abstract algebra. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about abstract algebra. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='abstract_algebra',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_machine_learning',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about machine learning. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about machine learning. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='machine_learning',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_clinical_knowledge',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about clinical knowledge. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about clinical knowledge. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='clinical_knowledge',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_global_facts',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about global facts. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about global facts. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='global_facts',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_management',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about management. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about management. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='management',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_nutrition',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about nutrition. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about nutrition. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='nutrition',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_marketing',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about marketing. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about marketing. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='marketing',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_professional_accounting',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about professional accounting. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about professional accounting. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='professional_accounting',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_geography',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school geography. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school geography. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_geography',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_international_law',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about international law. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about international law. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='international_law',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_moral_scenarios',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about moral scenarios. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about moral scenarios. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='moral_scenarios',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_computer_security',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about computer security. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about computer security. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='computer_security',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_microeconomics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school microeconomics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school microeconomics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_microeconomics',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_professional_law',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about professional law. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about professional law. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='professional_law',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_medical_genetics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about medical genetics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about medical genetics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='medical_genetics',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_professional_psychology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about professional psychology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about professional psychology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='professional_psychology',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_jurisprudence',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about jurisprudence. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about jurisprudence. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='jurisprudence',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_world_religions',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about world religions. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about world religions. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='world_religions',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_philosophy',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about philosophy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about philosophy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='philosophy',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_virology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about virology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about virology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='virology',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_chemistry',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school chemistry. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school chemistry. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_chemistry',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_public_relations',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about public relations. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about public relations. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='public_relations',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_macroeconomics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school macroeconomics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school macroeconomics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_macroeconomics',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_human_sexuality',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about human sexuality. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about human sexuality. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='human_sexuality',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_elementary_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about elementary mathematics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about elementary mathematics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='elementary_mathematics',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school physics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school physics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_physics',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_computer_science',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school computer science. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school computer science. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_computer_science',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_european_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school european history. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school european history. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_european_history',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_business_ethics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about business ethics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about business ethics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='business_ethics',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_moral_disputes',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about moral disputes. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about moral disputes. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='moral_disputes',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_statistics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school statistics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school statistics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_statistics',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_miscellaneous',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about miscellaneous. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about miscellaneous. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='miscellaneous',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_formal_logic',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about formal logic. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about formal logic. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='formal_logic',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_government_and_politics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school government and politics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school government and politics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_government_and_politics',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_prehistory',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about prehistory. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about prehistory. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='prehistory',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_security_studies',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about security studies. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about security studies. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='security_studies',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_biology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school biology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school biology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_biology',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_logical_fallacies',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about logical fallacies. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about logical fallacies. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='logical_fallacies',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_world_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school world history. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school world history. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_world_history',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_professional_medicine',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about professional medicine. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about professional medicine. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='professional_medicine',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_mathematics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school mathematics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school mathematics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_mathematics',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_college_medicine',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about college medicine. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about college medicine. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='college_medicine',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_us_history',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school us history. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school us history. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_us_history',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_sociology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about sociology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about sociology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='sociology',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_econometrics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about econometrics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about econometrics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='econometrics',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_high_school_psychology',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about high school psychology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about high school psychology. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='high_school_psychology',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_human_aging',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about human aging. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about human aging. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='human_aging',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_us_foreign_policy',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about us foreign policy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about us foreign policy. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='us_foreign_policy',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
    dict(
        abbr='lukaemon_mmlu_conceptual_physics',
        eval_cfg=dict(
            evaluator=dict(
                type='opencompass.openicl.icl_evaluator.AccwithDetailsEvaluator'
            ),
            pred_postprocessor=dict(
                options='ABCD',
                type=
                'opencompass.utils.text_postprocessors.first_option_postprocess'
            )),
        infer_cfg=dict(
            ice_template=dict(
                template=dict(round=[
                    dict(
                        prompt=
                        'There is a single choice question about conceptual physics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                        role='HUMAN'),
                    dict(prompt='{target}\n', role='BOT'),
                ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            inferencer=dict(
                type='opencompass.openicl.icl_inferencer.GenInferencer'),
            prompt_template=dict(
                ice_token='</E>',
                template=dict(
                    begin='</E>',
                    round=[
                        dict(
                            prompt=
                            'There is a single choice question about conceptual physics. Answer the question by replying A, B, C or D.\nQuestion: {input}\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: ',
                            role='HUMAN'),
                    ]),
                type='opencompass.openicl.icl_prompt_template.PromptTemplate'),
            retriever=dict(
                fix_id_list=[
                    0,
                    1,
                    2,
                    3,
                    4,
                ],
                type='opencompass.openicl.icl_retriever.FixKRetriever')),
        name='conceptual_physics',
        path='opencompass/mmlu',
        reader_cfg=dict(
            input_columns=[
                'input',
                'A',
                'B',
                'C',
                'D',
            ],
            output_column='target',
            test_range=10,
            train_split='dev'),
        type='opencompass.datasets.MMLUDataset'),
]
infer = dict(
    partitioner=dict(type='opencompass.partitioners.NaivePartitioner'),
    runner=dict(
        max_num_workers=4,
        task=dict(type='opencompass.tasks.OpenICLInferTask'),
        type='opencompass.runners.LocalRunner'))
models = [
    dict(
        abbr='Qwen3-8B',
        batch_size=16,
        is_chat=True,
        key='EMPTY',
        max_out_len=2048,
        max_seq_len=4096,
        meta_template=None,
        openai_api_base='http://127.0.0.1:8000/v1/chat/completions',
        path='Qwen3-8B',
        query_per_second=64,
        run_cfg=dict(num_gpus=0),
        temperature=0.0,
        type='opencompass.models.OpenAIExtra'),
]
work_dir = 'outputs/qwen3_eval_result/20250603_073810'
