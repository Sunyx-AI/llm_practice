{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b58ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 自动下载模型时，指定使用modelscope; 否则，会从HuggingFace下载\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'True'\n",
    "\n",
    "def get_completion(prompts, model, tokenizer=None, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_tokens=4096, max_model_len=8192):\n",
    "    stop_token_ids = [151645, 151643]\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        min_p=min_p,\n",
    "        max_tokens=max_tokens,\n",
    "        stop_token_ids=stop_token_ids\n",
    "    )\n",
    "    llm = LLM(model=model, tokenizer=tokenizer, max_model_len=max_model_len,trust_remote_code=True)  # max_model_len 用于限制模型在推理过程中可以处理的最大输入和输出长度之和。\n",
    "    outputs = llm.generate(prompts, sampling_params=sampling_params)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "394bb6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 06:00:46 [__init__.py:31] Available plugins for group vllm.general_plugins:\n",
      "INFO 06-03 06:00:46 [__init__.py:33] - lora_filesystem_resolver -> vllm.plugins.lora_resolvers.filesystem_resolver:register_filesystem_resolver\n",
      "INFO 06-03 06:00:46 [__init__.py:36] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.\n",
      "INFO 06-03 06:00:56 [config.py:793] This model supports multiple tasks: {'generate', 'score', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 06-03 06:00:56 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 06-03 06:00:58 [core.py:438] Waiting for init message from front-end.\n",
      "INFO 06-03 06:00:58 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='/root/Qwen/Qwen/Qwen3-8B', speculative_config=None, tokenizer='/root/Qwen/Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/Qwen/Qwen/Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}\n",
      "WARNING 06-03 06:00:58 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fec01d20e50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W603 06:01:19.583397663 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W603 06:01:40.596964188 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[rank0]:[W603 06:01:40.622080964 ProcessGroupGloo.cpp:727] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 06:01:41 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 06-03 06:01:41 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-03 06:01:41 [gpu_model_runner.py:1531] Starting to load model /root/Qwen/Qwen/Qwen3-8B...\n",
      "INFO 06-03 06:01:41 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "ERROR 06-03 06:01:41 [core.py:500] EngineCore failed to start.\n",
      "ERROR 06-03 06:01:41 [core.py:500] Traceback (most recent call last):\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 491, in run_engine_core\n",
      "ERROR 06-03 06:01:41 [core.py:500]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 390, in __init__\n",
      "ERROR 06-03 06:01:41 [core.py:500]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\n",
      "ERROR 06-03 06:01:41 [core.py:500]     self.model_executor = executor_class(vllm_config)\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "ERROR 06-03 06:01:41 [core.py:500]     self._init_executor()\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "ERROR 06-03 06:01:41 [core.py:500]     self.collective_rpc(\"load_model\")\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 06-03 06:01:41 [core.py:500]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/utils.py\", line 2605, in run_method\n",
      "ERROR 06-03 06:01:41 [core.py:500]     return func(*args, **kwargs)\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 164, in load_model\n",
      "ERROR 06-03 06:01:41 [core.py:500]     self.model_runner.load_model()\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1534, in load_model\n",
      "ERROR 06-03 06:01:41 [core.py:500]     self.model = get_model(vllm_config=self.vllm_config)\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 58, in get_model\n",
      "ERROR 06-03 06:01:41 [core.py:500]     return loader.load_model(vllm_config=vllm_config,\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 273, in load_model\n",
      "ERROR 06-03 06:01:41 [core.py:500]     model = initialize_model(vllm_config=vllm_config,\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/utils.py\", line 61, in initialize_model\n",
      "ERROR 06-03 06:01:41 [core.py:500]     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/qwen3.py\", line 270, in __init__\n",
      "ERROR 06-03 06:01:41 [core.py:500]     self.model = Qwen3Model(vllm_config=vllm_config,\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "ERROR 06-03 06:01:41 [core.py:500]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/qwen3.py\", line 242, in __init__\n",
      "ERROR 06-03 06:01:41 [core.py:500]     super().__init__(vllm_config=vllm_config,\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "ERROR 06-03 06:01:41 [core.py:500]     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 320, in __init__\n",
      "ERROR 06-03 06:01:41 [core.py:500]     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 625, in make_layers\n",
      "ERROR 06-03 06:01:41 [core.py:500]     [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 626, in <listcomp>\n",
      "ERROR 06-03 06:01:41 [core.py:500]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 322, in <lambda>\n",
      "ERROR 06-03 06:01:41 [core.py:500]     lambda prefix: decoder_layer_type(config=config,\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/qwen3.py\", line 188, in __init__\n",
      "ERROR 06-03 06:01:41 [core.py:500]     self.mlp = Qwen3MLP(\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 82, in __init__\n",
      "ERROR 06-03 06:01:41 [core.py:500]     self.down_proj = RowParallelLinear(\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 1199, in __init__\n",
      "ERROR 06-03 06:01:41 [core.py:500]     self.quant_method.create_weights(\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 189, in create_weights\n",
      "ERROR 06-03 06:01:41 [core.py:500]     weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "ERROR 06-03 06:01:41 [core.py:500]   File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "ERROR 06-03 06:01:41 [core.py:500]     return func(*args, **kwargs)\n",
      "ERROR 06-03 06:01:41 [core.py:500] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 43.75 MiB is free. Process 9464 has 20.05 GiB memory in use. Process 19135 has 3.59 GiB memory in use. Of the allocated memory 3.30 GiB is allocated by PyTorch, and 1.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 504, in run_engine_core\n",
      "    raise e\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 491, in run_engine_core\n",
      "    engine_core = EngineCoreProc(*args, **kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 390, in __init__\n",
      "    super().__init__(vllm_config, executor_class, log_stats,\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core.py\", line 71, in __init__\n",
      "    self.model_executor = executor_class(vllm_config)\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 52, in __init__\n",
      "    self._init_executor()\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 47, in _init_executor\n",
      "    self.collective_rpc(\"load_model\")\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/utils.py\", line 2605, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/worker/gpu_worker.py\", line 164, in load_model\n",
      "    self.model_runner.load_model()\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1534, in load_model\n",
      "    self.model = get_model(vllm_config=self.vllm_config)\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 58, in get_model\n",
      "    return loader.load_model(vllm_config=vllm_config,\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/default_loader.py\", line 273, in load_model\n",
      "    model = initialize_model(vllm_config=vllm_config,\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/model_loader/utils.py\", line 61, in initialize_model\n",
      "    return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/qwen3.py\", line 270, in __init__\n",
      "    self.model = Qwen3Model(vllm_config=vllm_config,\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/qwen3.py\", line 242, in __init__\n",
      "    super().__init__(vllm_config=vllm_config,\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/compilation/decorators.py\", line 151, in __init__\n",
      "    old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 320, in __init__\n",
      "    self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 625, in make_layers\n",
      "    [PPMissingLayer() for _ in range(start_layer)] + [\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/utils.py\", line 626, in <listcomp>\n",
      "    maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 322, in <lambda>\n",
      "    lambda prefix: decoder_layer_type(config=config,\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/qwen3.py\", line 188, in __init__\n",
      "    self.mlp = Qwen3MLP(\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 82, in __init__\n",
      "    self.down_proj = RowParallelLinear(\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 1199, in __init__\n",
      "    self.quant_method.create_weights(\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py\", line 189, in create_weights\n",
      "    weight = Parameter(torch.empty(sum(output_partition_sizes),\n",
      "  File \"/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/torch/utils/_device.py\", line 104, in __torch_function__\n",
      "    return func(*args, **kwargs)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 23.68 GiB of which 43.75 MiB is free. Process 9464 has 20.05 GiB memory in use. Process 19135 has 3.59 GiB memory in use. Of the allocated memory 3.30 GiB is allocated by PyTorch, and 1.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m      5\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      6\u001b[0m     {\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt\n\u001b[1;32m      9\u001b[0m     }\n\u001b[1;32m     10\u001b[0m ]\n\u001b[1;32m     11\u001b[0m text \u001b[38;5;241m=\u001b[39m tokenzier\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     12\u001b[0m     messages,\n\u001b[1;32m     13\u001b[0m     tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     14\u001b[0m     add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     enable_thinking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m   \u001b[38;5;66;03m# 是否开启思考模式\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mget_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 对于思考模式，官方建议使用以下参数：temperature = 0.6，TopP = 0.95，TopK = 20，MinP = 0。\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m     21\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mprompt\n",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mget_completion\u001b[0;34m(prompts, model, tokenizer, temperature, top_p, top_k, min_p, max_tokens, max_model_len)\u001b[0m\n\u001b[1;32m     10\u001b[0m stop_token_ids \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m151645\u001b[39m, \u001b[38;5;241m151643\u001b[39m]\n\u001b[1;32m     11\u001b[0m sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams(\n\u001b[1;32m     12\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m     13\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     stop_token_ids\u001b[38;5;241m=\u001b[39mstop_token_ids\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_model_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# max_model_len 用于限制模型在推理过程中可以处理的最大输入和输出长度之和。\u001b[39;00m\n\u001b[1;32m     20\u001b[0m outputs \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mgenerate(prompts, sampling_params\u001b[38;5;241m=\u001b[39msampling_params)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/utils.py:1183\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1178\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1179\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1180\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         )\n\u001b[0;32m-> 1183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/entrypoints/llm.py:253\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_token, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    224\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    225\u001b[0m     task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    250\u001b[0m )\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/engine/llm_engine.py:501\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMEngine \u001b[38;5;28;01mas\u001b[39;00m V1LLMEngine\n\u001b[1;32m    499\u001b[0m     engine_cls \u001b[38;5;241m=\u001b[39m V1LLMEngine\n\u001b[0;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mengine_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_vllm_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:123\u001b[0m, in \u001b[0;36mLLMEngine.from_vllm_config\u001b[0;34m(cls, vllm_config, usage_context, stat_loggers, disable_log_stats)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_vllm_config\u001b[39m(\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     disable_log_stats: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLLMEngine\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m               \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m               \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m               \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m               \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVLLM_ENABLE_V1_MULTIPROCESSING\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/llm_engine.py:100\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_processor \u001b[38;5;241m=\u001b[39m OutputProcessor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[1;32m     97\u001b[0m                                         log_stats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_stats)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core \u001b[38;5;241m=\u001b[39m \u001b[43mEngineCoreClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m multiprocess_mode:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# for v0 compatibility\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_core\u001b[38;5;241m.\u001b[39mengine_core\u001b[38;5;241m.\u001b[39mmodel_executor  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:75\u001b[0m, in \u001b[0;36mEngineCoreClient.make_client\u001b[0;34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AsyncMPClient(vllm_config, executor_class, log_stats)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:580\u001b[0m, in \u001b[0;36mSyncMPClient.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[1;32m    579\u001b[0m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m--> 580\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_queue \u001b[38;5;241m=\u001b[39m queue\u001b[38;5;241m.\u001b[39mQueue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;66;03m# Ensure that the outputs socket processing thread does not have\u001b[39;00m\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# a ref to the client which prevents gc.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:418\u001b[0m, in \u001b[0;36mMPClient.__init__\u001b[0;34m(self, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcore_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcore_engines[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Wait for engine core process(es) to start.\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mutility_results: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, AnyFuture] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# Request objects which may contain pytorch-allocated tensors\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# that we need to keep references to until zmq is done with the\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m# underlying data.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/vllm/v1/engine/core_client.py:484\u001b[0m, in \u001b[0;36mMPClient._wait_for_engine_startup\u001b[0;34m(self, output_address, parallel_config)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(events) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m events[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m sync_input_socket:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;66;03m# One of the local core processes exited.\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     finished \u001b[38;5;241m=\u001b[39m proc_manager\u001b[38;5;241m.\u001b[39mfinished_procs(\n\u001b[1;32m    483\u001b[0m     ) \u001b[38;5;28;01mif\u001b[39;00m proc_manager \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine core initialization failed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    485\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee root cause above. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[1;32m    489\u001b[0m eng_identity, ready_msg_bytes \u001b[38;5;241m=\u001b[39m sync_input_socket\u001b[38;5;241m.\u001b[39mrecv_multipart()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "model = '/root/Qwen/Qwen/Qwen3-8B'\n",
    "tokenzier = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "\n",
    "prompt = \"给我一个关于大模型的简短介绍。\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "text = tokenzier.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking = True   # 是否开启思考模式\n",
    ")\n",
    "\n",
    "outputs = get_completion(text, model, tokenizer=None, temperature=0.6, top_p = 0.95, top_k=20, min_p=0)  # 对于思考模式，官方建议使用以下参数：temperature = 0.6，TopP = 0.95，TopK = 20，MinP = 0。\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, \\nResponse: {generated_text!r}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb23a16b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
