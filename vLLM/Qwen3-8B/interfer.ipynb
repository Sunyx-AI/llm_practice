{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b58ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/envs/py310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 06:06:46 [__init__.py:243] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 06:06:48,741\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 自动下载模型时，指定使用modelscope; 否则，会从HuggingFace下载\n",
    "os.environ['VLLM_USE_MODELSCOPE'] = 'True'\n",
    "\n",
    "def get_completion(prompts, model, tokenizer=None, temperature=0.6, top_p=0.95, top_k=20, min_p=0, max_tokens=4096, max_model_len=8192):\n",
    "    stop_token_ids = [151645, 151643]\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        min_p=min_p,\n",
    "        max_tokens=max_tokens,\n",
    "        stop_token_ids=stop_token_ids\n",
    "    )\n",
    "    llm = LLM(model=model, tokenizer=tokenizer, max_model_len=max_model_len,trust_remote_code=True)  # max_model_len 用于限制模型在推理过程中可以处理的最大输入和输出长度之和。\n",
    "    outputs = llm.generate(prompts, sampling_params=sampling_params)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "394bb6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 06:13:09 [config.py:793] This model supports multiple tasks: {'score', 'classify', 'generate', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 06-03 06:13:09 [config.py:2118] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 06-03 06:13:10 [core.py:438] Waiting for init message from front-end.\n",
      "INFO 06-03 06:13:10 [core.py:65] Initializing a V1 LLM engine (v0.9.0.1) with config: model='/root/Qwen/Qwen/Qwen3-8B', speculative_config=None, tokenizer='/root/Qwen/Qwen/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/Qwen/Qwen/Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 512}\n",
      "WARNING 06-03 06:13:11 [utils.py:2671] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f0d202ccf40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W603 06:13:34.173447804 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[W603 06:13:57.196061017 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n",
      "[rank0]:[W603 06:13:57.267544441 ProcessGroupGloo.cpp:727] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 06:13:57 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 06-03 06:13:57 [topk_topp_sampler.py:58] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 06-03 06:13:57 [gpu_model_runner.py:1531] Starting to load model /root/Qwen/Qwen/Qwen3-8B...\n",
      "INFO 06-03 06:13:57 [cuda.py:217] Using Flash Attention backend on V1 engine.\n",
      "INFO 06-03 06:13:58 [backends.py:35] Using InductorAdaptor\n",
      "INFO 06-03 06:13:58 [backends.py:35] Using InductorAdaptor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  20% Completed | 1/5 [00:10<00:41, 10.39s/it]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 2/5 [00:11<00:13,  4.66s/it]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 3/5 [00:19<00:12,  6.38s/it]\n",
      "Loading safetensors checkpoint shards:  80% Completed | 4/5 [00:20<00:04,  4.14s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:20<00:00,  2.73s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 5/5 [00:20<00:00,  4.08s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-03 06:14:18 [default_loader.py:280] Loading weights took 20.42 seconds\n",
      "INFO 06-03 06:14:18 [gpu_model_runner.py:1549] Model loading took 15.2683 GiB and 20.705012 seconds\n",
      "INFO 06-03 06:14:28 [backends.py:459] Using cache directory: /root/.cache/vllm/torch_compile_cache/1cb62ff5bb/rank_0_0 for vLLM's torch.compile\n",
      "INFO 06-03 06:14:28 [backends.py:469] Dynamo bytecode transform time: 9.66 s\n",
      "INFO 06-03 06:14:34 [backends.py:158] Cache the graph of shape None for later use\n",
      "INFO 06-03 06:15:11 [backends.py:170] Compiling a graph for general shape takes 41.97 s\n",
      "INFO 06-03 06:15:32 [monitor.py:33] torch.compile takes 51.63 s in total\n",
      "INFO 06-03 06:15:33 [kv_cache_utils.py:637] GPU KV cache size: 31,072 tokens\n",
      "INFO 06-03 06:15:33 [kv_cache_utils.py:640] Maximum concurrency for 8,192 tokens per request: 3.79x\n",
      "INFO 06-03 06:16:06 [gpu_model_runner.py:1933] Graph capturing finished in 33 secs, took 0.59 GiB\n",
      "INFO 06-03 06:16:06 [core.py:167] init engine (profile, create kv cache, warmup model) took 107.30 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 323.29it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:08<00:00,  8.83s/it, est. speed input: 2.04 toks/s, output: 46.32 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: '<|im_start|>user\\n给我一个关于大模型的简短介绍。<|im_end|>\\n<|im_start|>assistant\\n', \n",
      "Response: '<think>\\n好的，用户让我提供一个关于大模型的简短介绍。首先，我需要明确用户的需求是什么。他们可能对大模型不太了解，或者需要一个快速概述。我应该先确定大模型的基本定义，包括它们的规模、技术基础和应用场景。\\n\\n接下来，我需要考虑用户可能的背景。他们可能是学生、研究人员，或者对AI技术感兴趣的普通用户。如果是学生，可能需要更基础的解释；如果是研究人员，可能需要更深入的技术细节。但用户要求的是“简短介绍”，所以应该保持简洁，避免过于技术化的术语。\\n\\n然后，我需要涵盖大模型的关键点：参数量、训练数据、应用场景、优势和挑战。比如，大模型通常有数十亿到万亿参数，基于深度学习，使用大量文本数据训练，能够处理多种任务如语言理解、生成、翻译等。优势包括强大的泛化能力，但挑战如计算资源需求高、数据隐私问题等。\\n\\n还要注意结构是否清晰，分点是否有助于理解。可能需要使用项目符号或简短段落。同时，要确保语言通俗易懂，避免专业术语过多，必要时进行解释。\\n\\n另外，用户可能想知道大模型的实际应用，比如在自然语言处理、图像识别、医疗诊断等领域的例子。但因为要简短，可能需要提到几个主要应用领域即可。\\n\\n最后，检查是否有遗漏的重要信息，比如大模型的发展趋势，如多模态模型、小模型优化等，但可能超出简短介绍的范围。保持内容精炼，确保信息准确且全面。\\n</think>\\n\\n大模型（Large Language Models）是基于深度学习的自然语言处理技术，通过海量文本数据训练，具备强大的语言理解和生成能力。它们通常包含数十亿到万亿个参数，能完成文本生成、翻译、问答、编程等复杂任务。近年来，大模型在医疗、金融、教育等领域广泛应用，但也面临计算成本高、数据隐私和伦理挑战等问题。'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = '/root/Qwen/Qwen/Qwen3-8B'\n",
    "tokenzier = AutoTokenizer.from_pretrained(model, use_fast=True)\n",
    "\n",
    "prompt = \"给我一个关于大模型的简短介绍。\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": prompt\n",
    "    }\n",
    "]\n",
    "text = tokenzier.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking = True   # 是否开启思考模式\n",
    ")\n",
    "\n",
    "outputs = get_completion(text, model, tokenizer=None, temperature=0.6, top_p = 0.95, top_k=20, min_p=0)  # 对于思考模式，官方建议使用以下参数：temperature = 0.6，TopP = 0.95，TopK = 20，MinP = 0。\n",
    "\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Prompt: {prompt!r}, \\nResponse: {generated_text!r}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854c1456",
   "metadata": {},
   "source": [
    "## 创建兼容OpenAI API接口的服务器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17bb04",
   "metadata": {},
   "source": [
    "`Qwen3-8B`是兼容`OpenAI API`协议，因此可以直接使用`vLLM`创建`OpenAI服务器`，默认会创建在 http://localhost:8000, 服务器当前一次托管一个模型，并实现列表模型、`completions`和`Chat completions`端口\n",
    "- `completions`：是基于文本生成任务,模型会在给定一段体时候生成一段文本,适用于生成文章、故事和邮件\n",
    "- `Chat completions`：面向对话任务, 模型需要理解和生成对话。用于对话机器人或对话系统\n",
    "创建服务器可以指定模型磨成、模型路径、聊天模版等参数\n",
    "- `--host`和`--port`参数指定地址\n",
    "- `--model` 参数制定模型名称\n",
    "- `--chat-template` 指定聊天模版\n",
    "- `--served-model-name` 指定服务模型的名称。\n",
    "- `--max-model-len` 指定模型的最大长度。\n",
    "- `--enable-reasoning` 开启思考模式\n",
    "- `--reasoning-parser` 指定如何解析模型生成的推理内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb23a16b",
   "metadata": {},
   "source": [
    "可以使用以下命令到终端上,启动Qwen3-8B模型的API接口\n",
    "```bash\n",
    "VLLM_USE_MODELSCOPE=true vllm serve /root/Qwen/Qwen/Qwen3-8B --served-model-name Qwen3-8B --max_model_len 8192 --enable-reasoning --reasoning-parser deepseek_r1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5127af",
   "metadata": {},
   "source": [
    "可以使用`curl`命令查看当前模型列表\n",
    "```bash\n",
    "curl http://localhost:8000/v1/models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46bcbc",
   "metadata": {},
   "source": [
    "## 使用python脚本请求OpenAI Completions API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bad25803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='\\n\\n5的阶乘（记作5!）是将1到5的所有正整数相乘的结果：\\n\\n$$\\n5! = 5 \\\\times 4 \\\\times 3 \\\\times 2 \\\\times 1 = 120\\n$$\\n\\n**答案：120**', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content='\\n嗯，用户问的是5的阶乘是多少。首先，我得确认阶乘的定义。阶乘就是从1乘到那个数，比如n的阶乘是n×(n-1)×...×1。所以5的阶乘应该是5×4×3×2×1对吧？\\n\\n不过，我得仔细算一遍，别算错了。先算5乘4是20，然后乘3是60，接着乘2是120，最后乘1还是120。对吧？嗯，没错，应该是120。\\n\\n不过，有没有可能用户问的是别的什么？比如，有时候可能会有其他数学概念被混淆，但阶乘一般就是这个定义。再想想，有没有可能用户输入有误？比如是不是问的是5的某个其他运算？不过问题明确说“阶乘”，所以应该没问题。\\n\\n再检查一遍计算步骤，确保没有遗漏。5×4=20，没错；20×3=60，对的；60×2=120，没错；120×1=120。是的，没错。所以答案应该是120。\\n\\n或者有没有可能用户对阶乘的定义不太清楚？比如，是否包括0？不过0的阶乘是1，但这里问的是5，所以没问题。总之，答案应该是120。\\n')\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"sk-xxx\"\n",
    ")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"Qwen3-8B\",\n",
    "    messages=[\n",
    "         {\"role\": \"user\", \"content\": \"我想问你，5的阶乘是多少？\"}\n",
    "    ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67446256",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
